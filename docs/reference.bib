@article{Klein2006,
author = {Klein, Gary and Moon, Brian and Hoffman, Robert and Associates, Klein},
doi = {10.1109/MIS.2006.100},
file = {:Users/dorislee/Box/Papers/sensemaking.2.pdf:pdf},
isbn = {15411672},
issn = {1541-1672},
journal = {IEEE Intelligent Systems},
number = {5},
pages = {88--92},
pmid = {22655693},
title = {{A Macrocognitive Model Human-Centered Computing A Macrocognitive Model}},
url = {http://cmaps.perigeantechnologies.com/rid=1GC9J5HBH-29R587Q-42M/Sensemaking.2.pdf},
volume = {21},
year = {2006}
}

@unknown{Pirolli2005,
author = {Pirolli, Peter and Card, Stuart},
year = {2005},
month = {01},
pages = {},
title = {The sensemaking process and leverage points for analyst technology as identified through cognitive task analysis}
}

@article{Roy2015,
abstract = {With the increased generation and availability of big data in different domains, there is an imminent requirement for data analysis tools that are able to ‘explain' the trends and anomalies obtained from this data to a range of users with different backgrounds. Wu-Madden (PVLDB 2013) and Roy-Suciu (SIGMOD 2014) recently proposed solutions that can explain interesting or unexpected answers to sim- ple aggregate queries in terms of predicates on attributes. In this paper, we propose a generic framework that can sup- port much richer, insightful explanations by preparing the database offline, so that top explanations can be found inter- actively at query time. The main idea in such explanation- ready databases is to pre-compute the effects of potential ex- planations (called interventions), and efficiently re-evaluate the original query taking into account these effects. We formalize this notion and define an explanation-query that can evaluate all possible explanations simultaneously with- out having to run an iterative process, develop algorithms and optimizations, and evaluate our approach with experi- ments on real data.},
author = {Roy, Sudeepa and Orr, Laurel and Suciu, Dan},
doi = {10.14778/2856318.2856329},
file = {:Users/dorislee/Box/Papers/VLDB2016-ExplanationReady.pdf:pdf},
issn = {21508097},
journal = {Proceedings of the VLDB Endowment},
mendeley-groups = {Database Usability},
number = {4},
pages = {348--359},
title = {{Explaining query answers with explanation-ready databases}},
volume = {9},
year = {2015}
}
@article{Chapman2009,
author = {Chapman, Adriane and Jagadish, H.V.},
doi = {10.1097/CCM.0b013e3181cab091},
file = {:Users/dorislee/Box/Papers/10.1.1.227.6569.pdf:pdf},
isbn = {1530-0293 (Electronic)$\backslash$r0090-3493 (Linking)},
journal = {SIGMOD 2009},
mendeley-groups = {Database Usability},
number = {2},
pages = {711--712},
pmid = {20083939},
title = {{Why not?}},
volume = {38},
year = {2009}
}
@article{Jagadish2007,
abstract = {Database researchers have striven to improve the capability of a database in terms of both performance and functionality. We assert that the usability of a database is as important as its capability. In this paper, we study why database systems today are so difficult to use. We identify a set of five pain points and propose a research agenda to address these. In particular, we introduce a presentation data model and recommend direct data manipulation with a schema later approach. We also stress the importance of provenance and of consistency across presentation models.},
author = {Jagadish, Hv and Chapman, Adriane},
doi = {10.1145/1247480.1247483},
file = {:Users/dorislee/Box/Papers/usability.pdf:pdf},
isbn = {9781595936868},
issn = {07308078},
journal = {Proceedings of the {\ldots}},
keywords = {database,usability,user interface},
mendeley-groups = {Database Usability},
number = {1},
pages = {13},
title = {{Making database systems usable}},
url = {http://portal.acm.org/citation.cfm?doid=1247480.1247483{\%}5Cnhttp://dl.acm.org/citation.cfm?id=1247483},
volume = {D},
year = {2007}
}
@article{Wu2013,
abstract = {Database users commonly explore large data sets by running ag-gregate queries that project the data down to a smaller number of points and dimensions, and visualizing the results. Often, such vi-sualizations will reveal outliers that correspond to errors or surpris-ing features of the input data set. Unfortunately, databases and vi-sualization systems do not provide a way to work backwards from an outlier point to the common properties of the (possibly many) unaggregated input tuples that correspond to that outlier. We pro-pose Scorpion, a system that takes a set of user-specified outlier points in an aggregate query result as input and finds predicates that explain the outliers in terms of properties of the input tuples that are used to compute the selected outlier results. Specifically, this explanation identifies predicates that, when applied to the in-put data, cause the outliers to disappear from the output. To find such predicates, we develop a notion of influence of a predicate on a given output, and design several algorithms that efficiently search for maximum influence predicates over the input data. We show that these algorithms can quickly find outliers in two real data sets (from a sensor deployment and a campaign finance data set), and run orders of magnitude faster than a naive search algorithm while providing comparable quality on a synthetic data set.},
author = {Wu, Eugene and Madden, Samuel},
doi = {10.14778/2536354.2536356},
file = {:Users/dorislee/Library/Application Support/Mendeley Desktop/Downloaded/Wu, Madden - 2013 - Scorpion Explaining Away Outliers in Aggregate Queries.pdf:pdf},
issn = {21508097},
journal = {Proceedings of the VLDB Endowment},
mendeley-groups = {Database Usability},
number = {8},
pages = {553--564},
title = {{Scorpion: Explaining Away Outliers in Aggregate Queries}},
volume = {6},
year = {2013}
}
@article{Khoussainova2010,
author = {Khoussainova, Nodira and Kwon, Yc},
doi = {10.14778/1880172.1880175},
file = {:Users/dorislee/Box/Papers/p22-khoussainova.pdf:pdf},
isbn = {2150-8097},
issn = {2150-8097},
journal = {Proceedings of the {\ldots}},
mendeley-groups = {Database Usability},
pages = {22--33},
title = {{Snipsuggest: Context-aware autocompletion for sql}},
url = {http://dl.acm.org/citation.cfm?id=1880175},
year = {2010}
}
@article{Nandi2013,
abstract = {Direct, ad-hoc interaction with databases has typically been per-$\backslash$nformed over console-oriented conversational interfaces using query$\backslash$nlanguages such as SQL. With the rise in popularity of gestural user$\backslash$ninterfaces and computing devices that use gestures as their exclusive$\backslash$nmodes of interaction, database query interfaces require a fundamen-$\backslash$ntal rethinking to work without keyboards. We present a novel query$\backslash$nspecification system that allows the user to query databases using a$\backslash$nseries of gestures. We present a novel gesture recognition system$\backslash$nthat uses both the interaction and the state of the database to classify$\backslash$ngestural input into relational database queries. We conduct exhaus-$\backslash$ntive systems performance tests and user studies to demonstrate that$\backslash$nour system is not only performant and capable of interactive laten-$\backslash$ncies, but it is also more usable, faster to use and more intuitive than$\backslash$nexisting systems.},
author = {Nandi, Arnab and Jiang, Lilong and Mandel, Michael},
doi = {10.14778/2732240.2732247},
file = {:Users/dorislee/Box/Papers/gesturedb.pdf:pdf},
issn = {21508097},
journal = {Proceedings of the VLDB Endowment},
mendeley-groups = {Database Usability},
number = {4},
pages = {289--300},
title = {{Gestural query specification}},
url = {http://dl.acm.org/citation.cfm?doid=2732240.2732247},
volume = {7},
year = {2013}
}
@article{Idreos2013,
abstract = {As we enter the era of data deluge, turning data into knowl- edge has become the major challenge across most sciences and businesses that deal with data. In addition, as we in- crease our ability to create data, more and more people are confronted with data management problems on a daily basis for numerous aspects of every day life. A fundamental need is data exploration through interactive tools, i.e., being able to quickly and effortlessly determine data and patterns of interest. However, modern database systems have not been designed with data exploration and usability in mind; they require users with expert knowledge and skills, while they react in a strict and monolithic way to every user request, resulting in correct answers but slow response times. In this paper, we introduce the vision of a new generation of data management systems, called dbTouch; our vision is to enable interactive and intuitive data exploration via data- base kernels which are tailored for touch-based exploration. No expert knowledge is needed. Data is represented in a visual format, e.g., a column shape for an attribute or a fat rectangle shape for a table, while users can touch those shapes and interact/query with gestures as opposed to firing complex SQL queries. The system does not try to consume all data; instead it analyzes only parts of the data at a time, continuously refining the answers and continuously reacting to user input. Every single touch on a data object can be seen as a request to run an operator or a collection of opera- tors over part of the data. Users react to running results and continuously adjust the data exploration - they continuously determine the data to be processed next by adjusting the di- rection and speed of a gesture, i.e., a collection of touches; the database system does not have control on the data flow anymore. We discuss the various benefits that dbTouch sys- tems bring for data analytics as well as the new and unique challenges for database research in combination with touch interfaces. In addition, we provide an initial architecture, implementation and evaluation (and demo) of a dbTouch prototype over IOs for IPad.},
author = {Idreos, Stratos and Liarou, Erietta},
file = {:Users/dorislee/Box/Papers/dbtouch.pdf:pdf},
journal = {Cidr},
mendeley-groups = {Database Usability},
title = {{dbTouch: Analytics at your Fingertips.}},
url = {http://oai.cwi.nl/oai/asset/21322/21322B.pdf},
year = {2013}
}
@article{Morton2014,
abstract = {We present a vision of next-generation visual analytics ser- vices. We argue that these services should have three related capabilities: support visual and interactive data exploration as they do today, but also suggest relevant data to enrich visualizations, and facilitate the integration and cleaning of that data. Most importantly, they should provide all these capabilities seamlessly in the context of an uninterrupted data analysis cycle. We present the challenges and opportu- nities in building next-generation visual analytics services},
author = {Morton, Kristi and Balazinska, Magdalena and Grossman, Dan and Mackinlay, Jock},
doi = {10.14778/2732279.2732282},
file = {:Users/dorislee/Library/Application Support/Mendeley Desktop/Downloaded/Morton et al. - 2014 - Support the Data Enthusiast Challenges for Next-Generation Data-Analysis Systems.pdf:pdf},
issn = {21508097},
journal = {Proceedings of the VLDB Endowment, Volume 7, pp. 453–456, 2014},
keywords = {challenges for,kristi morton,magdalena balazinska,next-generation data-analysis systems,port the data enthusiast},
mendeley-groups = {HILDA/Viz/Theory/Reviews,Database Usability},
pages = {453--456},
title = {{Support the Data Enthusiast: Challenges for Next-Generation Data-Analysis Systems}},
url = {http://homes.cs.washington.edu/{~}kmorton/p446-morton.pdf},
volume = {7},
year = {2014}
}
@article{Yang2009,
abstract = {Complex databases are challenging to explore and query by users unfamiliar with their schemas. Enterprise databases often have hundreds of inter-linked tables, so even when extensive documentation is available, new users must spend a considerable amount of time understanding the schema before they can retrieve any information from the database. The problem is aggravated if the documentation is missing or outdated, which may happen with legacy databases. In this paper we identify limitations of previous approaches to address this vexing problem, and propose a principled approach to summarizing the contents of a relational database, so that a user can determine at a glance the type of information it contains, and the main tables in which that information resides. Our approach has three components: First, we define the importance of each table in the database as its stable state value in a random walk over the schema graph, where the transition probabilities depend on the entropies of table attributes. This ensures that the importance of a table depends both on its information content, and on how that content relates to the content of other tables in the database. Second, we define a metric space over the tables in a database, such that the distance function is consistent with an intuitive notion of table similarity. Finally, we use a Weighted k-Center algorithm under this distance function to cluster all tables in the database around the most relevant tables, and return the result as our summary. We conduct an extensive experimental study on a benchmark database, comparing our approach with previous methods, as well as with several hybrid models. We show that our approach not only achieves significantly higher accuracy than the previous state of the art, but is also faster and scales linearly with the size of the schema graph.},
author = {Yang, Xiaoyan and Procopiuc, Cecilia M and Srivastava, Divesh},
doi = {10.14778/1687627.1687699},
file = {:Users/dorislee/Box/Papers/vldb09-784.pdf:pdf},
isbn = {0000000000000},
issn = {21508097},
journal = {Vldb},
mendeley-groups = {Database Usability},
number = {1},
pages = {634--645},
title = {{Summarizing relational databases}},
url = {http://portal.acm.org/citation.cfm?id=1687699{\&}dl=GUIDE,},
volume = {2},
year = {2009}
}
@article{Joglekar2015,
abstract = {We present a data exploration system equipped with smart drill-down, a novel operator for interactively exploring a relational table to discover and summarize " interesting " groups of tuples. Each such group of tuples is represented by a rule. For instance, the rule (a, b, 1000) tells us that there are a thousand tuples with value a in the first column and b in the second column (and any value in the third column). Smart drill-down presents an analyst with a list of rules that together describe interesting aspects of the table. The analyst can tailor the definition of interesting, and can interactively apply smart drill-down on an existing rule to explore that part of the table. In the demonstration, conference attendees will be able to use the data exploration system equipped with smart drill-down, and will be able to contrast smart drill-down to traditional drill-down, for various interestingness measures, and resource constraints.},
author = {Joglekar, Manas and Garcia-molina, Hector and Parameswaran, Aditya},
file = {:Users/dorislee/Box/Papers/p1928-joglekar.pdf:pdf},
issn = {21508097},
journal = {Proceedings of the 41st International Conference on Very Large Data Bases},
mendeley-groups = {Database Usability},
number = {12},
pages = {1928--1931},
title = {{Smart Drill-Down : A New Data Exploration Operator}},
volume = {8},
year = {2015}
}
@article{Vartak2015,
author = {Vartak, Manasi and Madden, Samuel and Parmeswaran, Aditya N},
file = {:Users/dorislee/Library/Application Support/Mendeley Desktop/Downloaded/Vartak, Madden, Parmeswaran - 2015 - SEEDB Supporting Visual Analytics with Data-Driven Recommendations.pdf:pdf},
mendeley-groups = {HILDA,Database Usability},
title = {{SEEDB : Supporting Visual Analytics with Data-Driven Recommendations}},
year = {2015}
}

@article{Abouzied2012,
abstract = {Writing complex queries in SQL is a challenge for users. Prior work has developed several techniques to ease query specification but none of these techniques are applicable to a particularly difficult class of queries: quantified queries. Our hypothesis is that users prefer to specify quantified queries interactively by trial-and-error. We identify two impediments to this form of interactive trial-and-error query specification in SQL: (i) changing quantifiers often requires global syntactical query restructuring, and (ii) the absence of non-answers from SQL's results makes verifying query correctness difficult. We remedy these issues with DataPlay, a query tool with an underlying graphical query language, a unique data model and a graphical interface. DataPlay provides two interaction features that support trial-and-error query specification. First, DataPlay allows users to directly manipulate a graphical query by changing quantifiers and modifying dependencies between constraints. Users receive real-time feedback in the form of updated answers and non-answers. Second, DataPlay can auto-correct a user's query, based on user feedback about which tuples to keep or drop from the answers and non-answers. We evaluated the effectiveness of each interaction feature with a user study and we found that direct query manipulation is more effective than auto-correction for simple queries but auto-correction is more effective than direct query manipulation for more complex queries.},
author = {Abouzied, Azza and Hellerstein, J and Silberschatz, A},
doi = {10.1145/2380116.2380144},
file = {:Users/dorislee/Library/Application Support/Mendeley Desktop/Downloaded/Abouzied, Hellerstein, Silberschatz - 2012 - Dataplay interactive tweaking and example-driven correction of graphical database queries.pdf:pdf},
isbn = {9781450315807},
journal = {Proceedings of the 25th annual ACM symposium on User interface software and technology},
pages = {207--217},
title = {{Dataplay: interactive tweaking and example-driven correction of graphical database queries}},
url = {http://dl.acm.org/citation.cfm?id=2380144},
year = {2012}
}
